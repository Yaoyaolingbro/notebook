# Improve Policy Gradient

## Introduce Q,V into Policy Gradient

æˆ‘ä»¬é‡æ–°è€ƒè™‘ä¹‹å‰çš„è€ƒè™‘causalityçš„ policy gradientçš„è¡¨è¾¾å¼ï¼š

$$
\nabla_\theta J(\theta)=\mathbb{E}_{\tau \sim p_{\bar{\theta}}(\tau)}\left[\sum_{t=1}^T\left(\nabla_{\theta}\log \pi_\theta(a_t|s_t)\sum_{t'=t}^Tr(s_{t'},a_{t'})\right)\right]=\mathbb{E}_{\tau \sim p_{\bar{\theta}}(\tau)}\left[\sum_{t=1}^T\nabla_{\theta}\log \pi_\theta(a_t|s_t)\hat{Q}^{\pi_\theta}_{n,t}
\right]
$$

ä½†è¿™ä¸ªè¡¨è¾¾å¼å…·æœ‰æ¯”è¾ƒå¤§çš„varianceã€‚ä»”ç»†ä¸€æƒ³ï¼Œæ˜¯å› ä¸ºå¯¹äºæ¯ä¸€æ¬¡ $\hat{Q}^{\pi_\theta}_{n,t}$ æˆ‘ä»¬åªè®¡ç®—ä¸€æ¡è½¨è¿¹ï¼Œè¿™ä¼šå¯¼è‡´varianceå¾ˆå¤§ã€‚è¯·è®°ä½æœ¬è®²çš„takeoverï¼š

> **é‡è¦æ€æƒ³.** å¯¹äºå¾ˆå¤šé—®é¢˜ï¼Œæœ‰ä¸¤ç§æ–¹æ¡ˆï¼š
>
> 1. ä¸€ä¸ªå•é‡‡æ ·çš„ä¼°è®¡ï¼Œè™½ç„¶unbiasedä½†æ–¹å·®å¤§ï¼›
> 2. ä¸€ä¸ªæ¨¡å‹çš„æ‹Ÿåˆï¼Œè™½ç„¶å¯¹ä¸å‡†ç¡®çš„modelæœ‰ä¸€å®šbiasï¼Œä½†æ–¹å·®å°ã€‚
>
> æˆ‘ä»¬éœ€è¦åœ¨è¿™ä¸¤è€…ä¹‹é—´åšä¸€ä¸ªtradeoffã€‚

æœ¬è®²å¾ˆå¤šåœ°æ–¹éƒ½è¦é‡‡ç”¨è¿™ç§æ€æƒ³ã€‚æ¯”å¦‚è¿™é‡Œï¼Œæ ¹æ®è¿™ä¸ª**é‡è¦æ€æƒ³**ï¼Œæˆ‘ä»¬å¸Œæœ›æŠŠ $\hat{Q}^{\pi_\theta}_{n,t}$ æ¢æˆå¾ˆå¤šè½¨è¿¹çš„æœŸå¾…å€¼ã€‚

é‚£è¿™ä¸ªæœŸå¾…å€¼æ˜¯ä»€ä¹ˆï¼Ÿæˆ‘ä»¬å‘ç°ï¼Œå…¶å®å°±æ˜¯

$$
Q^{\pi_\theta}(s_t,a_t)=\mathbb{E}_{\tau_{\ge t} \sim p_{\bar{\theta}}(\tau_{\ge t}|s_t,a_t)}\left[\sum_{t'=t}^Tr(s_{t'},a_{t'})\right]=\mathbb{E}_{\tau_{\ge t} \sim p_{\bar{\theta}}(\tau_{\ge t}|s_t,a_t)}\left[\hat{Q}^{\pi_\theta}_{n,t}\right]
$$

æ‰€ä»¥ä¸€ä¸ªæ›´å¥½çš„ï¼ˆvarianceæ›´å°ï¼‰çš„è¡¨è¾¾å¼æ˜¯

$$
\nabla_\theta J(\theta)=\mathbb{E}_{\tau \sim p_{\theta}(\tau)}\left[\sum_{t=1}^T\nabla_{\theta}\log \pi_\theta(a_t|s_t)Q^{\pi_\theta}(s_t,a_t)\right]
$$

### With Baseline

ä¸ºäº†è¿›ä¸€æ­¥å‡å°varianceï¼Œæˆ‘ä»¬å¯ä»¥å¼•å…¥ä¸€ä¸ª**baseline**ã€‚å®ƒçš„é€‰å–åº”è¯¥æ˜¯ $Q^{\pi_\theta}(s_t,a_t)$ çš„æŸç§å¹³å‡ï¼Œæˆ‘ä»¬ä¼šå‘ç°ç›¸æ¯”äºä¹‹å‰æœ€æ™®é€šçš„ç›´æ¥å¹³å‡ï¼Œæˆ‘ä»¬å¯ä»¥æœ‰

$$
V^{\pi_\theta}(s_t)=\mathbb{E}_{a_t\sim \pi_\theta(a_t|s_t)}\left[Q^{\pi_\theta}(s_t,a_t)\right]
$$

è¿™æ ·ï¼Œå°±æœ‰äº†ä¸€ä¸ªæ–°çš„è¡¨è¾¾å¼

$$
\nabla_\theta J(\theta)=\mathbb{E}_{\tau \sim p_{\theta}(\tau)}\left[\sum_{t=1}^T\nabla_{\theta}\log \pi_\theta(a_t|s_t)\left(Q^{\pi_\theta}(s_t,a_t)-V^{\pi_\theta}(s_t)\right)\right]
$$

è¿™é‡Œçš„ $Q^{\pi_\theta}(s_t,a_t)-V^{\pi_\theta}(s_t)$ ä¹Ÿè¢«ç§°ä½œAdvantage Functionï¼ˆè®°ä½œ $A^{\pi_\theta}(s_t,a_t)$ ï¼‰ï¼šå®ƒä»£è¡¨åšaction $a_t$ ç›¸æ¯”äºå¹³å‡æƒ…å†µçš„ä¼˜åŠ¿ã€‚

## Fitting Q and V

æ˜¾ç„¶ï¼Œæ—¢ç„¶å¼•å…¥äº†Qå’ŒVï¼Œæˆ‘ä»¬å°±éœ€è¦ç½‘ç»œæ¥ä¼°è®¡ $Q$ å’Œ $V$ ã€‚ä¸è¿‡ï¼Œæˆ‘ä»¬ä¸€å¼€å§‹æ¥æƒ³ï¼Œå¹¶ä¸å¸Œæœ›åŒæ—¶å¼•å…¥ä¸¤ä¸ªç½‘ç»œæ¥æ‹Ÿåˆï¼Œè¿™æ˜¯å› ä¸ºä»–ä»¬æœ¬èº«å°±æœ‰è¾ƒç®€å•çš„å…³ç³»ï¼Œè€Œä¸”æˆ‘ä»¬ä¹Ÿå¸Œæœ›å°½é‡å‡å°ç®—åŠ›çš„å¼€é”€ã€‚ä¸è¿‡ï¼Œè¿™å¾ˆå¥½å®ç°ï¼Œå› ä¸ºæˆ‘ä»¬å¯ä»¥è¡¨è¾¾ï¼š

$$
Q^{\pi_\theta}(s_t,a_t)-V^{\pi_\theta}(s_t) = Q^{\pi_\theta}(s_t,a_t)-\mathbb{E}_{a_t\sim \pi_\theta(a_t|s_t)}\left[Q^{\pi_\theta}(s_t,a_t)\right]
$$

ä¹Ÿå¯ä»¥å†™å‡º

$$
Q^{\pi_\theta}(s_t,a_t)-V^{\pi_\theta}(s_t)= r(s_t,a_t)+\mathbb{E}_{s_{t+1}\sim p(s_{t+1}|s_t,a_t)}[V^{\pi_\theta}(s_{t+1})]-V^{\pi_\theta}(s_t)
$$

å› æ­¤ï¼Œæ— è®ºæ˜¯æˆ‘ä»¬é€‰æ‹©æ‹Ÿåˆ $Q$ è¿˜æ˜¯ $V$ ï¼Œæˆ‘ä»¬éƒ½å¯ä»¥ç›´æ¥å¸¦å…¥åˆ°ä¹‹å‰çš„policy gradientå¸¦æ¥çš„è¡¨è¾¾å¼ä¸­ï¼Œè¿›è¡Œè®­ç»ƒã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬è®¨è®ºçš„é—®é¢˜å°±æ˜¯ç©¶ç«Ÿæ‹Ÿåˆ $Q$ è¿˜æ˜¯ $V$ äº†ã€‚

æˆ‘ä»¬ä»¥$V$ç½‘ç»œçš„æ‹Ÿåˆä¸ºä¾‹å­ã€‚æˆ‘ä»¬çŸ¥é“ï¼Œ$V^{\pi_\theta}(s_{n,t})$çš„ç›®æ ‡æ˜¯

$$
y_{n,t}=\sum_{t'=t}^Tr(s_{n,t'},a_{n,t'})
$$

ä¸€ä¸ªè‡ªç„¶çš„æƒ³æ³•æ˜¯ï¼Œæˆ‘ä»¬å»è®­ç»ƒ

$$
L(\phi)=\frac{1}{N}\sum_{n=1}^N\sum_{t=1}^{T}\left(y_{n,t}-V^{\pi_\theta}_{\phi}(s_{n,t})\right)^2
$$

ä½†æ˜¯é—®é¢˜æ˜¯ï¼Œè¿™æ ·ç›¸å½“äºé‡‡é›†ä¸€å †å•é‡‡æ ·çš„æ ·æœ¬å½“æˆè®­ç»ƒæ•°æ®ï¼Œå¯ä»¥æƒ³è±¡åˆ°æ–¹å·®ä¾ç„¶å¾ˆå¤§ï¼Œé‡‡æ ·çš„æ•°ç›®ä¹Ÿæ²¡æœ‰å‡å°‘ã€‚ä¸€ä¸ªé‡è¦çš„æ€æƒ³æ˜¯ï¼Œæˆ‘ä»¬å»ºç«‹ä¸€ä¸ª**é€’æ¨å…³ç³»**ã€‚æ¯”å¦‚ç°åœ¨ï¼Œæˆ‘ä»¬è¿‘ä¼¼åœ°ç»™å‡º

$$
V^{\pi_\theta}(s_{n,t})\leftarrow y_{n,t}=\sum_{t'=t}^Tr(s_{n,t'},a_{n,t'})\approx r(s_{n,t},a_{n,t}) + \mathbb{E}_{s_{t+1}\sim p(s_{t+1}|s_{n,t},a_{n,t})}[V^{\pi_\theta}(s_{t+1})]
$$

è¿™æ ·ï¼Œæˆ‘ä»¬å°±å–å¾—äº†å·¨å¤§çš„é£è·ƒï¼šæˆ‘ä»¬å•é‡‡æ ·çš„å˜é‡ä»ä¸€ç³»åˆ—$a_{n,t},s_{n+1,t},\cdots$åˆ°åªæœ‰ä¸€ä¸ª$a_{n,t}$ï¼è¿™æ ·ï¼Œæˆ‘ä»¬å¯ä»¥æ–™æƒ³åˆ°æˆ‘ä»¬çš„æ–¹å·®å‡å°‘äº†ï¼›ä½†æ˜¯æ ¹æ®**é‡è¦æ€æƒ³**ï¼Œæˆ‘ä»¬å¯¹åº”çš„ä»£ä»·æ˜¯targetä¹ŸåŒ…å«æˆ‘ä»¬æ­£åœ¨è®­ç»ƒçš„ç½‘ç»œï¼Œå› æ­¤å¢å¤§äº†biasã€‚

ç±»ä¼¼åœ°ï¼Œå¯¹$Q$æ˜¯ä¸æ˜¯ä¹Ÿå¯ä»¥åšä¸€æ ·çš„æ“ä½œå‘¢ï¼Ÿæˆ‘ä»¬å‘ç°å¯è¡Œï¼š

$$
Q^{\pi_\theta}(s_{n,t},a_{n,t})\leftarrow r(s_{n,t},a_{n,t})+\mathbb{E}_{s_{t+1}\sim p(s_{t+1}|s_{n,t},a_{n,t}),a_{t+1}\sim \pi_\theta(a_{t+1}|s_{t+1})}[Q^{\pi_\theta}(s_{t+1},a_{t+1})]
$$

ä½†æ˜¯$Q$çš„ç¼ºç‚¹åœ¨äºï¼Œæˆ‘ä»¬å¯ä»¥å‘ç°ï¼Œè¿™é‡Œå¿…é¡»åšä¸¤ä¸ªé‡‡æ ·ï¼ˆ$s_{t+1}$å’Œ$a_{t+1}$ï¼‰ï¼Œä¹Ÿå°±æ˜¯æ–¹å·®ä¼šç›¸å¯¹æ›´å¤§ã€‚

å› æ­¤ï¼Œæˆ‘ä»¬åœ¨è¿™é‡Œé€‰æ‹©**æ‹Ÿåˆ $V$**ã€‚

### Policy Evaluation

æ‰€è°“Policy Evaluationå°±æ˜¯æŒ‡ï¼Œæˆ‘ä»¬æ ¹æ®ä¸€ä¸ªpolicy ç»™å‡ºå…¶å¯¹åº”çš„value functionï¼Œè¿›ä¸€æ­¥ç»™å‡ºadvantageã€‚æ ¹æ®å‰é¢çš„è®¨è®ºï¼Œ$V$ çš„ç›®æ ‡å˜æˆäº†

$$
\hat{L}(\phi)=\frac{1}{N}\sum_{n=1}^N\sum_{t=1}^{T}\left(\text{SG}[\hat{y}_{n,t}]-V^{\pi_\theta}_{\phi}(s_{n,t})\right)^2=\frac{1}{N}\sum_{n=1}^N\sum_{t=1}^{T}\left(r(s_{n,t},a_{n,t})+\text{SG}[V^{\pi_\theta}_{\phi}(s_{n,t+1})]-V^{\pi_\theta}_{\phi}(s_{n,t})\right)^2
$$

ï¼ˆæ³¨æ„è¿™é‡Œstop gradientï¼ŒåŸå› æ ¹æ®æˆ‘ä»¬çš„æ¨ç†è¿‡ç¨‹ï¼Œå‰ä¸€ä¸ª$V$æ˜¯ä½œä¸ºtargetå‡ºç°ï¼Œæ‰€ä»¥ä¸åº”è¯¥è¢«updateï¼Œä¹Ÿå°±æ˜¯è¿™é‡Œæˆ‘ä»¬å°±æ˜¯ä¸€ä¸ªç®€å•çš„MSE lossã€‚ï¼‰è¿™ä¸ªæœ€ç»ˆçš„è¡¨è¾¾å¼å°±æ˜¯æˆ‘ä»¬çš„policy evaluationçš„è®­ç»ƒç›®æ ‡ã€‚

## Summary

ç®€å•æ€»ç»“ä¸€ä¸‹ï¼Œæˆ‘ä»¬ç©¶ç«Ÿä½œäº†å“ªäº›äº‹æƒ…ï¼š

- å¼•å…¥Q,Væ¥ä»£æ›¿policy gradientä¸­çš„rewardæ±‚å’Œï¼Œå‡å°varianceï¼›
- é€šè¿‡è¿‘ä¼¼çš„æ–¹å¼ï¼Œé¿å…äº†å¯¹Qçš„æ‹Ÿåˆï¼ˆè™½ç„¶è¿™æ­¥ä¼šå¢å¤§varianceï¼‰ï¼›
- é€šè¿‡å·§å¦™è®¾è®¡æ‹ŸåˆVçš„ç›®æ ‡ï¼Œå†æ¬¡å‡å°varianceã€‚

# Actor-Critic Algorithm

æŠŠå‰é¢çš„æ–¹æ³•æ€»ç»“ä¸€ä¸‹ï¼Œæˆ‘ä»¬å°±å¾—åˆ°äº†**Actor-Criticç®—æ³•**ï¼š

1. åˆ©ç”¨ç°åœ¨çš„policy $\pi_\theta$ å– $N$ ä¸ªtrajectoryï¼›
2. ç”¨è¿™äº›æ•°æ®è®­ç»ƒ $V^{\pi_\theta}_{\phi}$ ï¼›
3. è®¡ç®— $A^{\pi_\theta}(s_t,a_t)=V_\phi^{\pi_\theta}(s_{t+1})-V_\phi^{\pi_\theta}(s_{t})+r(s_t,a_t)$
4. è®¡ç®— $\nabla_\theta J(\theta)=\frac{1}{N}\sum_{n}\left[\sum_{t=1}^T\nabla_{\theta}\log \pi_\theta(a_t|s_t)A^{\pi_\theta}(s_t,a_t)\right]$
5. ç”¨è¿™ä¸ªæ¢¯åº¦æ›´æ–° $\theta$

ç¬¬äºŒæ­¥å°±æ˜¯æˆ‘ä»¬å‰é¢è®¨è®ºçš„policy evaluationï¼Œå¯ä»¥é‡‡ç”¨å‰é¢çš„ä¸¤ç§æ–¹æ³•ä¸­ä»»ä½•ä¸€ç§ã€‚

## Discount Factor

å¦‚æœç®€å•æŒ‰ç…§å‰é¢çš„æ–¹æ³•è®­ç»ƒï¼Œæˆ‘ä»¬ç†æƒ³æƒ…å†µçš„ $V^{\pi_\theta}_\phi$ åº”è¯¥å°±æ˜¯

$$
V^{\pi_\theta}_\phi(s_t)=\mathbb{E}_{\tau_{\ge t} \sim p_{\bar{\theta}}(\tau_{\ge t}|s_t)}\left[\sum_{t'=t}^Tr(s_{t'},a_{t'})\right]
$$

ä½†è¿™ä¸ªæ±‚å’Œå¯¹äºinfinite horizonçš„æƒ…å†µæ˜¯å‘æ•£çš„ï¼å› æ­¤æˆ‘ä»¬å¼•å…¥ä¸€ä¸ªdiscount factor $\gamma\in [0,1)$ ï¼Œé‡æ–°å®šä¹‰æˆ‘ä»¬çš„ç›®æ ‡

$$
V^{\pi_\theta}_\phi(s_t):=\mathbb{E}_{\tau_{\ge t} \sim p_{\bar{\theta}}(\tau_{\ge t}|s_t)}\left[\sum_{t'=t}^T\gamma^{t'-t}r(s_{t'},a_{t'})\right]
$$

è¿™ä¼šå¸¦æ¥å¾ˆå¤šåœ°æ–¹çš„æ”¹å˜ã€‚

> **å°è´´å£«**
>
> æœ¬è®²æœ‰ä¸€äº›åœ°æ–¹å®Œå…¨æ”¹å˜äº†åŸæ¥çš„ç®—æ³•ã€‚ä¸ºäº†ä¿è¯ç†è§£ï¼Œå»ºè®®æ¯ä¸€æ¬¡è¿™ç§åœ°æ–¹éƒ½è‡ªå·±å…ˆæ€è€ƒä¸€ä¸‹å¼•å…¥çš„æ–°ä¿®æ”¹ä¼šå¯¼è‡´åŸæ¥ç®—æ³•å“ªäº›éƒ¨åˆ†çš„æ”¹å˜ï¼Œç„¶åå†çœ‹æˆ‘ä»¬çš„è§£é‡Šã€‚

åœ¨è¿™é‡Œï¼š

- $V^{\pi_\theta}_\phi$ çš„è®­ç»ƒè¦æ”¹å˜
- $A^{\pi_\theta}(s_t,a_t)$ çš„è®¡ç®—è¦æ”¹å˜

### $V^{\pi_\theta}_\phi$ çš„è®­ç»ƒ

å…·ä½“åœ°ï¼Œæˆ‘ä»¬

$$
{y}_{n,t}=r(s_{n,t},a_{n,t})+\sum_{t'=t+1}^T\gamma^{t'-t}r(s_{n,t'},a_{n,t'})\approx r(s_{n,t},a_{n,t})+\gamma V^{\pi_\theta}_{\phi}(s_{n,t+1}):=\hat{y}_{n,t}
$$

å› æ­¤è¦ä¿®æ”¹è®­ç»ƒæ–¹å¼ï¼š

$$
L(\phi)=\frac{1}{N}\sum_{n=1}^N\sum_{t=1}^{T}\left(\hat{y}_{n,t}-V^{\pi_\theta}_{\phi}(s_{n,t})\right)^2=\frac{1}{N}\sum_{n=1}^N\sum_{t=1}^{T}\left(r(s_{n,t},a_{n,t})+\gamma V^{\pi_\theta}_{\phi}(s_{n,t+1})-V^{\pi_\theta}_{\phi}(s_{n,t})\right)^2
$$

### $A^{\pi_\theta}(s_t,a_t)$ çš„è®¡ç®—

$$
Q^{\pi_\theta}(s_t,a_t)=r(s_t,a_t)+\gamma\mathbb{E}_{\tau_{> t} \sim p_{\bar{\theta}}(\tau_{> t}|s_t,a_t)}\left[\sum_{t'=t+1}^Tr(s_{t'},a_{t'})\right]
=r(s_t,a_t)+\gamma\mathbb{E}_{s_{t+1}\sim p(s_{t+1}|s_t,a_t)}\left[V^{\pi_\theta}_\phi(s_{t+1})\right]
$$

è¿™æ ·

$$
A^{\pi_\theta}(s_t,a_t)=Q^{\pi_\theta}(s_t,a_t)-V^{\pi_\theta}_\phi(s_t)=r(s_t,a_t)+\gamma V^{\pi_\theta}_\phi(s_{t+1})-V^{\pi_\theta}_\phi(s_t)
$$

## Two Kinds of Discount Factor (Optional)

**Warning.** å¦‚æœå·²ç»æ„Ÿè§‰æœ‰ç‚¹æ™•äº†ï¼Œåƒä¸‡åˆ«çœ‹è¿™ä¸€èŠ‚ã€‚

æˆ‘ä»¬å›é¡¾ç¬¬ä¸€æ¬¡å¼•å…¥discount factorçš„è¡¨è¾¾å¼

$$
V^{\pi_\theta}_\phi(s_t):=\mathbb{E}_{\tau_{\ge t} \sim p_{\bar{\theta}}(\tau_{\ge t}|s_t)}\left[\sum_{t'=t}^T\gamma^{t'-t}r(s_{t'},a_{t'})\right]
$$

ç¨ä½œå˜å½¢ï¼Œå¯ä»¥å‘ç°è¿™å¯¹åº”ç€æ™®é€špolicy gradientä¸­ï¼Œæˆ‘ä»¬è¿™æ ·åŠ å…¥ $\gamma$ ï¼š

$$
\nabla_\theta J(\theta)=\mathbb{E}_{\tau \sim p_{\theta}(\tau)}\left[\sum_{t=1}^T\nabla_{\theta}\log \pi_\theta(a_t|s_t)\left(\sum_{t'=t}^T\gamma^{t'-t}r(s_{t'},a_{t'})\right)\right]
$$

é‚£å½“ç„¶ä¹Ÿä¼šæœ‰å¦å¤–ä¸€ç§å¯èƒ½çš„åŠ å…¥æ–¹å¼ï¼š

$$
\nabla_\theta J'(\theta)=\mathbb{E}_{\tau \sim p_{\theta}(\tau)}\left[\sum_{t=1}^T\nabla_{\theta}\log \pi_\theta(a_t|s_t)\left(\sum_{t'=t}^T\gamma^{t'-1}r(s_{t'},a_{t'})\right)\right]
$$

è¿™ç§åŠ æ’‡å·çš„æ–¹å¼ï¼ˆç§°ä¸º**æ–¹å¼2**ï¼‰ä¹Ÿæœ‰å…¶é“ç†ï¼šæˆ‘ä»¬ç›¸å½“äºç›´æ¥æŠŠrewardè®°ä½œ

$$
R=\sum_{t=1}^T\gamma^{t-1}r(s_t,a_t)
$$

è€ŒåŸå…ˆçš„**æ–¹å¼1**ç”šè‡³å†™ä¸å‡ºä¸€ä¸ªåƒè¿™æ ·çš„ï¼Œé€šç”¨çš„rewardå½¢å¼ï¼è¿™å¾ˆå¤šæ—¶å€™ä¼šé€ æˆè¯¯è§£ï¼Œè®¤ä¸ºæ–¹å¼2æ˜¯æ­£ç¡®çš„ã€‚ä½†å®é™…ä¸Šå®Œå…¨ä¸æ˜¯ï¼š

**åœ¨å¤§éƒ¨åˆ†é—®é¢˜ä¸­ï¼Œdiscount factorçš„æ­£ç¡®åŠ å…¥æ–¹å¼æ˜¯æ–¹å¼1ã€‚**

æˆ‘ä»¬å¯ä»¥æ¥ç®€å•è®ºè¿°ä¸€ä¸‹ä¸ºä»€ä¹ˆã€‚è¿™éœ€è¦ç†è§£ä¸¤ç§æ–¹å¼è¡¨è¾¾çš„æ„ä¹‰ã€‚

æ–¹å¼2çš„å½¢å¼å®é™…ä¸Šæ›´åŠ ç›´è§‚ï¼šè¿™å¯ä»¥ç›´æ¥ç†è§£ä¸ºMDPåŠ å…¥ä¸€ä¸ª**dead state**ï¼šæ¯ä¸€æ¬¡æœ‰ $1-\gamma$ çš„æ¦‚ç‡æ­»æ‰ï¼Œæ­»æ‰ä¹‹ååªèƒ½ä¿æŒåœ¨dead stateï¼Œç„¶årewardå…¨éƒ¨ä¸º0ã€‚è¿™æ ·ï¼Œæˆ‘ä»¬å°±å¯ä»¥ç†è§£ $R$ çš„å½¢å¼ï¼›åŒæ—¶ï¼Œæˆ‘ä»¬å¯ä»¥å†™å‡ºå¯¹åº”çš„value functionï¼š

$$
V^{\pi_\theta}_\phi(s_t)=\mathbb{E}_{\tau_{\ge t} \sim p_{\bar{\theta}}(\tau_{\ge t}|s_t)}\left[\sum_{t'=t}^T\gamma^{t'-1}r(s_{t'},a_{t'})\right]=\gamma^{t-1}\mathbb{E}_{\tau_{\ge t} \sim p_{\bar{\theta}}(\tau_{\ge t}|s_t)}\left[\sum_{t'=t}^T\gamma^{t'-t}r(s_{t'},a_{t'})\right]
$$

å¯ä»¥çœ‹å‡ºï¼Œå³ä½¿åé¢çš„æ­¥éª¤ï¼ˆ $t$ å¾ˆå¤§ï¼‰ä½œå‡ºäº†æ¯”è¾ƒå¥½çš„å†³ç­–ï¼Œç”±äº $\gamma$ çš„å­˜åœ¨ï¼Œè¿™ä¸ªvalue functionä¹Ÿä¼šå˜å¾—å¾ˆå°ã€‚åœ¨è¿™ç§è®¾å®šä¸‹ï¼Œä¹Ÿæ˜¯åˆç†çš„ï¼Œå› ä¸ºæ­»äº¡çš„å¨èƒé€¼è¿«æ¨¡å‹æ›´æ—©åœ°ä½œå‡ºå¥½çš„å†³ç­–ã€‚

è€Œæ–¹å¼1åˆ™ä»£è¡¨ç€ï¼Œ**discount factorå¹¶ä¸çœŸæ­£å­˜åœ¨äºMDPä¸­**ï¼Œè€Œåªæ˜¯value functionå®šä¹‰çš„ä¸€éƒ¨åˆ†ã€‚è¿˜å¥è¯è¯´ï¼Œå®ƒ**åªæ˜¯ä¸€ä¸ªæ•°å­¦å·¥å…·ï¼Œæ²¡æœ‰å¤ªå¤šå®é™…ä¸Šçš„æ„ä¹‰**ï¼›ç”šè‡³å¯ä»¥ç†è§£ä¸ºæˆ‘ä»¬ä¸ºäº†é¿å…å‘æ•£ä¹˜ä¸Šä¸€ä¸ª $\gamma$ ï¼Œä½†ç†è®ºä¸Šæœ€ååº”è¯¥å– $\gamma\to 1$ ã€‚

åœ¨è¿™æ ·çš„è®¾å®šä¸‹ï¼Œæ¯ä¸€æ­¥çš„åœ°ä½éƒ½æ˜¯ç­‰åŒçš„ï¼›ä½†å½“ä½ ç«™åœ¨ $t$ æ­¥ä¼°è®¡ä¹‹åçš„valueçš„æ—¶å€™ï¼Œæˆ‘ä»¬ç•¥å¾®å‡å°‘æ›´é åçš„æ­¥éª¤çš„æƒé‡ã€‚

å½“ç„¶ï¼Œä¸¤ç§æ–¹å¼ç©¶ç«Ÿå“ªç§æ­£ç¡®ï¼Œå¿…é¡»å…·ä½“é—®é¢˜å…·ä½“åˆ†æã€‚ä½†å¤§éƒ¨åˆ†é—®é¢˜ä¸­å¹¶æ²¡æœ‰æ‰€è°“çš„dead stateï¼Œå¹¶ä¸”åº”è¯¥å…·æœ‰æ—¶é—´å¹³ç§»ä¸å˜æ€§ã€‚å› æ­¤ï¼Œåœ¨è¿™äº›æƒ…å†µä¸‹ï¼Œæ–¹å¼1éƒ½æ˜¯æ­£ç¡®çš„ã€‚

# Actor-Critic in Practice

åŠ å…¥discount factorä¹‹åï¼Œæˆ‘ä»¬å°±å¾—åˆ°äº†æœ€åŸºæœ¬çš„batch actor-criticç®—æ³•ã€‚

> **Actor-Critic Algorithm**
1. åˆ©ç”¨ç°åœ¨çš„policy $\pi_\theta$ å– $N$ ä¸ªtrajectoryï¼›
2. ç”¨è¿™äº›æ•°æ®è®­ç»ƒ $V^{\pi_\theta}_{\phi}$ ï¼›
3. è®¡ç®— $A^{\pi_\theta}(s_t,a_t)=\gamma V_\phi^{\pi_\theta}(s_{t+1})-V_\phi^{\pi_\theta}(s_{t})+r(s_t,a_t)$ ï¼›
4. è®¡ç®— $\nabla_\theta J(\theta)=\frac{1}{N}\sum_{n}\left[\sum_{t=1}^T\nabla_{\theta}\log \pi_\theta(a_t|s_t)A^{\pi_\theta}(s_t,a_t)\right]$ ï¼›
5. ç”¨è¿™ä¸ªæ¢¯åº¦æ›´æ–° $\theta$ ã€‚

æ¥ä¸‹æ¥æˆ‘ä»¬ä»‹ç»ä¸€äº›æ”¹è¿›ã€‚

## Online Actor-Critic

æˆ‘ä»¬ä¹‹å‰çš„ç®—æ³•æ˜¯batchçš„ï¼Œå³æ¯æ¬¡éƒ½è¦æ”¶é›† $N$ æ¡è½¨è¿¹ã€‚èƒ½å¦æŠŠè¿™ä¸ªç®—æ³•å˜æˆä¸€ä¸ªonlineçš„ç®—æ³•å‘¢ï¼Ÿæ³¨æ„çš„ï¼Œæ™®é€šçš„policy gradientæ˜¯ç»å¯¹ä¸èƒ½å˜æˆonlineçš„ï¼Œå› ä¸ºå®ƒå¿…é¡»è®¡ç®— $t'\ge t$ çš„rewardæ±‚å’Œã€‚ä½†æ˜¯ç°åœ¨æˆ‘ä»¬çš„ç®—æ³•æ˜¯åŸºäºQ,Vçš„ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥å°è¯•æŠŠç®—æ³•å˜ä¸º

1. ç”¨å½“å‰çš„ç­–ç•¥ $\pi_\theta$ èµ°ä¸€æ­¥ï¼Œè®°ä¸º $\{s_t,a_t,s_{t+1},r=r(s_t,a_t)\}$ ï¼›
2. ç”¨ä¸€æ­¥çš„æ•°æ® $\{V_{\phi}^{\pi_\theta}(s_t),V_{\phi}^{\pi_\theta}(s_{t+1}),r\}$ è®­ç»ƒ $V^{\pi_\theta}_{\phi}$ ï¼›
3. è®¡ç®— $A^{\pi_\theta}(s_t,a_t)=\gamma V_\phi^{\pi_\theta}(s_{t+1})-V_\phi^{\pi_\theta}(s_{t})+r(s_t,a_t)$
4. è®¡ç®— $\nabla_\theta J(\theta)=\nabla_{\theta}\log \pi_\theta(a_t|s_t)A^{\pi_\theta}(s_t,a_t)$
5. ç”¨è¿™ä¸ªæ¢¯åº¦æ›´æ–° $\theta$

ä½†æˆ‘ä»¬å¾ˆå¿«å‘ç°è¿™å¤šäº†å¾ˆå¤šçš„å•é‡‡æ ·ï¼Œvarianceä¼šå¾ˆå¤§ã€‚è§£å†³æ–¹æ³•æ˜¯å¹¶è¡Œåœ°è¿›è¡Œå¾ˆå¤šä¸ªtrajectoryï¼Œç„¶åç”¨è¿™äº›trajectoryçš„å¹³å‡æ¥æ›´æ–°ã€‚å…·ä½“å®ç°æ–¹æ³•ä¸å†ä»‹ç»ã€‚

å¦‚æœä¸èƒ½å¹¶è¡Œï¼Œé‚£ä¹ˆæˆ‘ä»¬è¿˜æœ‰å¦å¤–ä¸€ç§æ–¹æ³•ï¼Œå°±æ˜¯ä¸‹é¢çš„**Off-policy Actor-Critic**ã€‚

## Off-policy Actor-Critic

é‡‡ç”¨**Replay Buffer**çš„æ–¹æ³•ä¹Ÿæœ‰åŠ©äºè§£å†³å‰é¢varianceè¾ƒå¤§çš„é—®é¢˜ã€‚å…·ä½“åœ°ï¼Œæˆ‘ä»¬è™½ç„¶æ¯ä¸€æ¬¡åªå’Œç¯å¢ƒäº¤äº’ä¸€æ¬¡ï¼Œä½†è¿™å¹¶ä¸ä»£è¡¨æˆ‘ä»¬åªèƒ½ç”¨è¿™ä¸€æ¬¡çš„æ•°æ®æ¥è®­ç»ƒã€‚æˆ‘ä»¬å¯ä»¥æŠŠè¿™äº›æ•°æ®å­˜å‚¨åœ¨ä¸€ä¸ªReplay Bufferé‡Œé¢ï¼Œç„¶åæ¯æ¬¡ä»è¿™ä¸ªBufferé‡Œé¢éšæœºå–ä¸€äº›æ•°æ®æ¥è®­ç»ƒã€‚

å› ä¸ºbufferé‡Œé¢çš„æ•°æ®å¹¶éå…¨éƒ¨æ¥è‡ªå½“å‰policyï¼Œæ‰€ä»¥è¿™ä¸ªæ–¹æ³•ä¹Ÿæ˜¯off-policyçš„ã€‚æˆ‘ä»¬ç§°è¿™ç§æ–¹æ³•ä¸º**Off-policy Actor-Critic**ã€‚ä½†æ˜¯å¼•å…¥replay bufferè‚¯å®šè¦æ¶‰åŠç®—æ³•çš„ä¿®æ”¹ã€‚

> ç°åœ¨ï¼Œè¯·ä½ æƒ³ä¸€æƒ³ï¼Œæœ‰å“ªäº›åœ°æ–¹éœ€è¦ä¿®æ”¹ï¼Ÿ

### ç¬¬2æ­¥çš„è®­ç»ƒ

æˆ‘ä»¬æœ¬æ¥æ‰“ç®—

$$
L(\phi)=\sum_{\text{batch}}\left(r(s_{t},a_{t})+\gamma V^{\pi_\theta}_{\phi}(s_{t+1})-V^{\pi_\theta}_{\phi}(s_{t})\right)^2
$$

ä½†æ˜¯æ³¨æ„åˆ°ç°åœ¨çš„ $a_t$ ä¸å†æ˜¯ä» $\pi_\theta$ ä¸­é‡‡æ ·å¾—åˆ°çš„ï¼ˆè€Œæ˜¯ä»replay bufferé‡Œé¢éšæœºæ‰¾å‡ºæ¥çš„ï¼‰ã€‚è¿™å°±å¯¼è‡´

$$
\mathbb{E}_{a_t\sim \textcolor{red}{\pi_{\text{old}}},s_{t+1}\sim p(s_{t+1}|s_t,a_t)}\left[r(s_t,a_t)+\gamma \hat{V}^{\pi_\theta}(s_{t+1})\right]\ne \hat{V} ^{\pi_\theta}(s_t)
$$

ï¼ˆå…¶ä¸­ $\hat{V}$ ä»£è¡¨çœŸå®çš„value functionï¼‰ï¼Œä¹Ÿå°±æ˜¯ä¸Šé¢çš„è®­ç»ƒç›®æ ‡å³ä½¿åœ¨ç†æƒ³æƒ…å†µä¹Ÿä¸åº”è¯¥æ˜¯0ã€‚

ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ‰ä¸€ä¸ªå·§å¦™çš„æ–¹æ³•ï¼šæˆ‘ä»¬**ä¸ç”¨ $V$ ï¼Œè€Œæ˜¯ç”¨ $Q$ æ¥è®­ç»ƒ**ï¼ç±»ä¼¼åœ°ï¼Œæˆ‘ä»¬å¯ä»¥å†™å‡ºä¸€ä¸ª $Q$ çš„ç›®æ ‡ï¼Œå¹¶ä¸”è¿™æ—¶å€™æ˜¯å¯¹çš„ï¼š

$$
\mathbb{E}_{s_{t+1}\sim p(s_{t+1}|s_t,a_t),a_{t+1}\sim \pi_\theta}\left[r(s_t,a_t)+\gamma \hat{Q}^{\pi_\theta}(s_{t+1},a_{t+1})\right]= \hat{Q} ^{\pi_\theta}(s_t,a_t)
$$

å› æ­¤ï¼Œç¬¬2æ­¥çš„è®­ç»ƒæ–¹æ³•å¯ä»¥ä¿®æ”¹ä¸ºï¼š

> ç”¨è¿™äº›æ•°æ®è®­ç»ƒ $Q^{\pi_\theta}_{\phi}$ ï¼Œå…·ä½“åœ°ï¼Œæœ€å°åŒ–

$$
L_{\text{new}}(\phi)=\sum_{\text{batch}}\left(r(s_{t},a_{t})+\gamma Q^{\pi_\theta}_{\phi}(s_{t+1},a_{t+1})-Q^{\pi_\theta}_{\phi}(s_{t},a_t)\right)^2
$$

> å…¶ä¸­ï¼Œ $a_{t+1}$ ä» $\pi_\theta$ ä¸­é‡‡æ ·ã€‚

### ç¬¬3æ­¥çš„è®¡ç®—

é¦–å…ˆï¼Œæˆ‘ä»¬è¿˜éœ€è¦è®¡ç®—advantage functionã€‚ç†è®ºä¸Šï¼Œå› ä¸ºç°åœ¨æœ‰çš„æ˜¯ $Q$ è€Œé $V$ ï¼Œæˆ‘ä»¬éœ€è¦è®¡ç®—

$$
A^{\pi_\theta}(s_t,a_t)=Q^{\pi_\theta}_\phi(s_t,a_t)-V^{\pi_\theta}(s_t)=Q^{\pi_\theta}_\phi(s_t,a_t)-\mathbb{E}_{a_t\sim \pi_\theta}\left[Q^{\pi_\theta}_\phi(s_t,a_t)\right]
$$

ä½†ä¼°è®¡è¿™ä¸ªå¹³å‡å€¼ç¨å¾®æœ‰äº›æ˜‚è´µï¼šå‡è®¾æˆ‘ä»¬é€‰å–çš„æ ·æœ¬ä¸å¤Ÿå¤šï¼Œå¾ˆå¯èƒ½å¯¹äºæ¯ä¸€ä¸ª $(s_t,a_t)$ å¾—åˆ°çš„baselineä¸ä¸€è‡´ï¼Œè¿™ä¼šé€ æˆå¾ˆå¤§çš„biasï¼›ä½†å¦‚æœé€‰å–å¤ªå¤šçš„æ ·æœ¬ï¼Œæ¯ä¸€ä¸ªè¿‡ä¸€æ¬¡ç½‘ç»œçš„æ—¶é—´å°±ä¼šå¾ˆé•¿ã€‚

å› æ­¤ï¼Œä¸€ä¸ªæ‘†çƒ‚çš„æ–¹æ³•æ˜¯ç”¨å¹²è„†ä¸è¦ $V$ äº†ï¼Œæˆ‘ä»¬ç›´æ¥é€‰

$$
A^{\pi_\theta}(s_t,a_t)=Q^{\pi_\theta}_\phi(s_t,a_t)
$$

è¿™æ ·è™½ç„¶æ²¡æœ‰baselineå‡å°çš„varianceï¼Œä½†æ˜¯æˆ‘ä»¬å¯ä»¥â€œè´¨é‡æ¢æ•°é‡â€ï¼Œç”¨æ›´å¤šçš„æ ·æœ¬æ¥å‡å°varianceã€‚

### ç¬¬4æ­¥çš„è®­ç»ƒ

æˆ‘ä»¬åŸæ¥çš„è¡¨è¾¾å¼æ˜¯

$$
\nabla_\theta J(\theta)=\sum_{\text{batch}}\nabla_{\theta}\log \pi_\theta(a_t|s_t)A^{\pi_\theta}(s_t,a_t)
$$

ä¹ä¸€çœ‹ï¼Œè¿™ä¸ªè¡¨è¾¾å¼ä¾ç„¶æˆç«‹ã€‚ä½†æ˜¯å®é™…ä¸Šå®Œå…¨ä¸æ˜¯ï¼šæˆ‘ä»¬éœ€è¦å›åˆ°åŸå…ˆpolicy gradientçš„æ¨å¯¼ï¼Œå°±ä¼šå‘ç°ï¼Œæˆ‘ä»¬å¿…é¡»ä¿è¯ $(s_t,a_t)$ å¯¹æ˜¯ä» $\pi_\theta$ çš„trajectoryä¸­é‡‡æ ·æ‰æ˜¯å¯¹çš„ã€‚å› æ­¤ï¼Œç°åœ¨æœ‰ä¸¤ä¸ªé—®é¢˜ï¼š

- $s_t$ çš„åˆ†å¸ƒï¼ˆç›¸å½“äºå¾ˆå¤šä¸åŒ $\pi_{\text{old}}$ åœ¨ç¬¬ $t$ æ­¥çš„åˆ†å¸ƒçš„å åŠ ï¼‰å’Œ $\pi_\theta$ åœ¨ç¬¬ $t$ æ­¥çš„çš„åˆ†å¸ƒä¸ä¸€è‡´ï¼›
- $a_t|s_t$ è¿™ä¸€åˆ†å¸ƒå¹¶ä¸æ˜¯ $\pi_\theta$

åè€…å¾ˆæ˜æ˜¾å¾ˆå¥½è§£å†³ï¼šæˆ‘ä»¬åªéœ€è¦ä» $\pi_\theta$ é‡Œé¢é‡æ–°é‡‡æ ·ä¸€ä¸ª $\tilde{a_t}$ å°±å¯ä»¥ã€‚ä½†æ˜¯å‰è€…å°±æ¯”è¾ƒéº»çƒ¦ã€‚æˆ‘ä»¬è¿™é‡Œé€‰å–å†æ¬¡æ‘†çƒ‚çš„æ–¹æ³•ï¼šæˆ‘ä»¬ç›´æ¥**ä¸ç®¡**è¿™ä¸ªé—®é¢˜ã€‚

> æŠ›å¼€ä¸€äº›ç»†èŠ‚ä¸è°ˆï¼Œè¿™ä¸ªè¿˜æ˜¯åˆç†çš„ï¼šæˆ‘ä»¬ç›¸å½“äºè®­ç»ƒæˆ‘ä»¬çš„ç­–ç•¥è¦åœ¨ä¸ç®¡å¦‚ä½•çš„åˆå§‹åˆ†å¸ƒ $p_{\pi}(s_t)$ ä¸‹éƒ½èƒ½åšçš„å¾ˆå¥½ï¼Œè¿™ç›¸å½“äºä¸€ä¸ªæ›´éš¾çš„è¦æ±‚ï¼Œä½†æ˜¯æ˜¯sufficientçš„ã€‚æˆ‘ä»¬ç›¸ä¿¡æˆ‘ä»¬çš„æ¨¡å‹èƒ½å®Œæˆè¿™ä¸ªæ›´éš¾çš„ä»»åŠ¡ğŸ˜Š

### Summary

æœ€ç»ˆï¼Œæˆ‘ä»¬ç»™å‡ºäº†off-policyçš„actor-criticå®ç°æ–¹å¼ã€‚

> **Off-Policy Actor-Critic Algorithm**

1. åˆ©ç”¨ç°åœ¨çš„policy $\pi_\theta$ èµ°ä¸€æ­¥ï¼Œå¾—åˆ° $\{s_t,a_t,s_{t+1},r(s_t,a_t)\}$ ï¼Œå­˜å…¥Replay Bufferï¼›
2. ä»Replay Bufferä¸­éšæœºå–ä¸€ä¸ªbatchï¼Œç”¨è¿™äº›æ•°æ®è®­ç»ƒ $Q^{\pi_\theta}_{\phi}$ ï¼ˆè§å‰é¢çš„ç›®æ ‡å‡½æ•°ï¼‰ã€‚è¿™ä¸ªè¿‡ç¨‹ä¸­ï¼Œå¯¹æ¯ä¸€ç»„æ•°æ®æˆ‘ä»¬éœ€è¦é‡‡æ ·ä¸€ä¸ªæ–°çš„action $a_{t+1}$ ï¼›
3. å– $A^{\pi_\theta}(s_t,a_t)=Q^{\pi_\theta}_\phi(s_t,a_t)$ ï¼›
4. è®¡ç®— $\nabla_\theta J(\theta)=\sum_{\text{batch}}\nabla_{\theta}\log \pi_\theta(\tilde{a_t}|s_t)A^{\pi_\theta}(s_t,\tilde{a_t})$ ï¼Œå…¶ä¸­ $\tilde{a_t}$ æ˜¯ä» $\pi_\theta$ ä¸­é‡‡æ ·å¾—åˆ°çš„ï¼›
5. ç”¨è¿™ä¸ªæ¢¯åº¦æ›´æ–° $\theta$ ã€‚

## Implementation Details

æ— è®ºæ˜¯å‰é¢çš„batch actor-criticè¿˜æ˜¯off-policy actor-criticï¼Œéƒ½å’Œå®è·µä¸­é‡‡ç”¨çš„ç‰ˆæœ¬ç›¸å·®ç”šè¿œã€‚æ›´é«˜çº§çš„å¤„ç†æ€è·¯ä¼šåœ¨åé¢ä»‹ç»ï¼ˆæˆ–è€…ä¸ä¼šè¢«ä»‹ç»ï¼‰ï¼Œè¿™é‡Œæˆ‘ä»¬åªä»‹ç»ä¸€ä¸ªæœ‰è¶£çš„å°ç»†èŠ‚â€”â€”Net Architectureã€‚

å¯ä»¥çœ‹åˆ°ï¼Œä¹‹å‰çš„ç®—æ³•ä¸­ï¼Œæˆ‘ä»¬éœ€è¦è®­ç»ƒä¸€ä¸ª $V$ ï¼ˆæˆ– $Q$ ï¼‰ç½‘ç»œå’Œä¸€ä¸ª $\pi$ ç½‘ç»œã€‚è¿™ä¸¤ä¸ªç½‘ç»œéƒ½è¾“å…¥ $s_t$ ã€‚è‡ªç„¶åœ°ï¼Œæˆ‘ä»¬ä¼šæƒ³åˆ°ï¼Œèƒ½ä¸èƒ½æŠŠè¿™ä¸¤ä¸ªç½‘ç»œåˆå¹¶æˆä¸€ä¸ªç½‘ç»œå‘¢ï¼Ÿè¿™æ˜¾ç„¶æ˜¯å¥½çš„ï¼Œå› ä¸ºæ¯”å¦‚ $s_t$ æ˜¯ä¸€å¼ å›¾ç‰‡ï¼Œé‚£ä¹ˆæå–ä¿¡æ¯çš„å·ç§¯æ ¸å°±å¯ä»¥è¢«å…±äº«ã€‚æœ€åï¼Œåªéœ€è¦åŠ å…¥ä¸¤ä¸ªä¸åŒçš„projection headå°±å¯ä»¥äº†ã€‚è¿™ç§°ä¸º**Shared Network Design**ã€‚

å½“ç„¶ï¼Œå®¹æ˜“çœ‹å‡ºï¼Œè¿™ä¸¤ä¸ªç½‘ç»œçš„å…¬å…±éƒ¨åˆ†ä¼šåƒæ¥è‡ªä¸¤ä¸ªä¸åŒæ¥æºçš„æ¢¯åº¦ã€‚è¿™ä¼šä½¿å¾—è®­ç»ƒä¸ç¨³å®šï¼Œä¹Ÿä½¿å¾—hyperparameterçš„é€‰æ‹©å˜å¾—æ›´åŠ å›°éš¾ã€‚å®é™…æƒ…å†µä¸­ï¼Œæˆ‘ä»¬éœ€è¦å…·ä½“é—®é¢˜å…·ä½“åˆ†æã€‚

# State-dependent Baselines

å›é¡¾æœ¬è®²çš„å¼€å¤´ï¼Œæˆ‘ä»¬è¿˜è®°å¾—policy gradientçš„lossè¡¨è¾¾å¼ä¸º

$$
J_{\text{PG}}(\theta)=\mathbb{E}_{\tau\sim p_\theta(\tau)}\left[\sum_{t=1}^T\nabla_{\theta}\log \pi_\theta(a_t|s_t)\left(\sum_{t'=t}^T \gamma^{t'-t} r(s_{t'},a_{t'})\right)\right]
$$

æˆ‘ä»¬å½“æ—¶ä½œå‡ºçš„ç¬¬ä¸€æ­¥æ˜¯å‡å°‘varianceï¼Œå› æ­¤ç”¨æ¨¡å‹é¢„æµ‹çš„æœŸå¾…å€¼ $Q^{\pi_\theta}_\phi(s_t,a_t)$ å–ä»£å•é‡‡æ ·çš„ $\sum_{t'=t}^T \gamma^{t'-t} r(s_{t'},a_{t'})$ ã€‚ä½†æ˜¯å›é¡¾æˆ‘ä»¬å¾ˆæ—©ä¹‹å‰æåˆ°çš„**é‡è¦æ€æƒ³**ï¼Œè¿™äºŒè€…ä¹‹é—´å­˜åœ¨tradeoffï¼šè¿™æ ·å¼•å…¥äº†æ¨¡å‹ä¸å‡†ç¡®é€ æˆçš„biasã€‚

ä¸€ä¸ªæŠ˜ä¸­çš„æ–¹æ³•æ˜¯ï¼Œæˆ‘ä»¬ä¾ç„¶ä½¿ç”¨æ¨¡å‹ï¼Œä½†è¿™æ¬¡æ¨¡å‹**åªæ˜¯ä½œä¸ºbaseline**å‡ºç°ï¼š

$$
\nabla_\theta J(\theta)=\mathbb{E}_{\tau\sim p_\theta(\tau)}\left[\sum_{t=1}^T \nabla_{\theta}\log \pi_\theta(a_t|s_t)\left[\left(\sum_{t'=t}^T \gamma^{t'-t} r(s_{t'},a_{t'})\right)-V^{\pi_\theta}_\phi(s_t)\right]\right]
$$

ç›¸æ¯”äºç¬¬äº”è®²ä»‹ç»policy gradientçš„æ—¶å€™ç®€å•çš„å¹³å‡baselineï¼Œè¿™é‡Œæ¨¡å‹äº§ç”Ÿçš„çš„baselineæ˜¾ç„¶å¼ºå¤§å¤šäº†ï¼›åŒæ—¶ï¼Œå› ä¸ºæ¨¡å‹ä½œä¸ºbaselineï¼ˆæœ¬èº«ç†è®ºä¸Šä¸ä¼šå½±å“æœŸæœ›ï¼Œåªæ˜¯å‡å°‘æ–¹å·®ï¼‰å‡ºç°ï¼Œå…¶å¯¹æ¨¡å‹çš„biasè¦æ±‚ä¹Ÿå°çš„å¤šã€‚è¿™ä¸ªå·§å¦™çš„è®¾è®¡è¢«ç§°ä¸º**State-dependent Baselines**ã€‚

## Control Variates

é™¤äº†ä¾èµ–stateä¹‹å¤–ï¼Œæˆ‘ä»¬è¿˜å¯ä»¥è¿›ä¸€æ­¥æœ‰Control Variatesï¼šå®ƒæŒ‡çš„æ˜¯**åŒæ—¶ä¾èµ–actionå’Œstate**çš„baselineã€‚å®Œå…¨ç±»ä¼¼åœ°ï¼Œæˆ‘ä»¬å¯ä»¥æœ‰

$$
\nabla_\theta J_1(\theta)=\mathbb{E}_{\tau\sim p_\theta(\tau)}\left[\sum_{t=1}^T \nabla_{\theta}\log \pi_\theta(a_t|s_t)\left[\left(\sum_{t'=t}^T \gamma^{t'-t} r(s_{t'},a_{t'})\right)-Q^{\pi_\theta}_\phi(s_t,a_t)\right]\right]
$$

ä½†è¿™é‡Œç•¥æœ‰ä¸€ä¸ªå°é—®é¢˜ï¼šæ ¹æ®ä¹‹å‰policy gradientçš„æ¨å¯¼ï¼Œæˆ‘ä»¬åªèƒ½å¢åŠ ä¸€ä¸ªå¸¸æ•°ï¼Œè€Œä¸èƒ½å¢åŠ ä¸€ä¸ªå’Œ $a_t$ ç›¸å…³çš„å‡½æ•°ã€‚å› æ­¤ï¼ŒçœŸæ­£çš„lossåº”è¯¥æ˜¯

$$
\nabla_\theta J(\theta)=\mathbb{E}_{\tau\sim p_\theta(\tau)}\left[\sum_{t=1}^T \nabla_{\theta}\log \pi_\theta(a_t|s_t)\left[\left(\sum_{t'=t}^T \gamma^{t'-t} r(s_{t'},a_{t'})\right)-Q^{\pi_\theta}_\phi(s_t,a_t)\right]\right]+\mathbb{E}_{\tau\sim p_\theta(\tau)}\left[\sum_{t=1}^T \nabla_{\theta}\log \pi_\theta(a_t|s_t)Q^{\pi_\theta}_\phi(s_t,a_t)\right]
$$

å¼•å…¥ä¹‹å‰ $\hat{Q}^{\pi_\theta}_{t}$ çš„ç®€è®°ï¼Œæˆ‘ä»¬å¯ä»¥å†™å‡º

$$
\nabla_\theta J(\theta)=\mathbb{E}_{\tau\sim p_\theta(\tau)}\left[\sum_{t=1}^T \nabla_{\theta}\log \pi_\theta(a_t|s_t)\left(\hat{Q}^{\pi_\theta}_{t}-Q^{\pi_\theta}_\phi(s_t,a_t)\right)\right]+\mathbb{E}_{\tau\sim p_\theta(\tau)}\left[\sum_{t=1}^T \nabla_{\theta}\log \pi_\theta(a_t|s_t)Q^{\pi_\theta}_\phi(s_t,a_t)\right]
$$

ç›´åˆ°è¿™é‡Œï¼Œä½ ä¹Ÿè®¸è®¤ä¸ºè¿™åªæ˜¯æ²¡æœ‰æ„ä¹‰çš„æ’ç­‰å˜å½¢â€”â€”ç¬¬ä¸€é¡¹å‡å»ä¸€é¡¹ï¼Œç¬¬äºŒé¡¹åˆåŠ ä¸Šä¸€é¡¹ã€‚ä½†å…³é”®ä¹‹å¤„åœ¨äºï¼Œç¬¬ä¸€é¡¹åŒ…å«æœŸå¾…å€¼æ˜¯0çš„ $\hat{Q}^{\pi_\theta}_{t}-Q^{\pi_\theta}_\phi(s_t,a_t)$ ï¼Œå› æ­¤åº”è¯¥ç›¸æ¯”ç¬¬äºŒé¡¹ä¼šå°å¾ˆå¤šï¼›ä½†å…³é”®æ˜¯ç¬¬äºŒé¡¹ä¸å†å’Œç¯å¢ƒæœ‰å…³ç³»â€”â€”è¿™æ ·ï¼Œæˆ‘ä»¬å¯ä»¥æƒ³å–å¤šå°‘sampleé€‰å–å¤šå°‘sampleï¼

å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥ä¿®æ”¹ $\nabla_\theta J(\theta)$ çš„è®¡ç®—ï¼š
- **å–å°‘é‡çš„sampleè®¡ç®—å‡ºç¬¬ä¸€é¡¹**ï¼Œè™½ç„¶æ–¹å·®è¾ƒå¤§ä½†æ˜¯å› ä¸ºè¿™ä¸€é¡¹æœ¬èº«ç›¸å¯¹ç¬¬äºŒé¡¹å¾ˆå°ï¼Œå½±å“ä¸å¤§ï¼›
- **å–å¤§é‡çš„sampleè®¡ç®—ç¬¬äºŒé¡¹**ï¼Œå› ä¸ºè¿™ä¸€é¡¹çš„è®¡ç®—ç›¸æ¯”äº $\hat{Q}^{\pi_\theta}_{t}$ è€Œè¨€åªéœ€è¦ $(s_t,a_t)$ ï¼Œä¸éœ€è¦åé¢çš„è½¨è¿¹ï¼Œæ‰€ä»¥å¯ä»¥å–å¾ˆå¤šsampleåŒæ—¶ä¿æŒè®¡ç®—çš„é«˜æ•ˆæ€§ã€‚è¿™å°±æ˜¯Control Variatesçš„æ ¸å¿ƒæ€æƒ³ã€‚

# Hybrid Methods

åœ¨è¿™æœ€åçš„ä¸€éƒ¨åˆ†ï¼Œæˆ‘ä»¬è€ƒè™‘æŠŠpolicy gradientå’Œactor-criticç»“åˆèµ·æ¥ã€‚

## N-step returns

æˆ‘ä»¬å¯¹æ¯”ä¸€ä¸‹policy gradientå’Œactor-criticçš„lossï¼š

$$
\nabla_\theta J_{\text{PG}}(\theta)=\mathbb{E}_{\tau\sim p_\theta(\tau)}\left[\sum_{t=1}^T \nabla_{\theta}\log \pi_\theta(a_t|s_t)\left(\sum_{t'=t}^T \gamma^{t'-t} r(s_{t'},a_{t'})\right)\right]
$$

$$
\nabla_\theta J_{\text{AC}}(\theta)=\mathbb{E}_{\tau\sim p_\theta(\tau)}\left[\sum_{t=1}^T \nabla_{\theta}\log \pi_\theta(a_t|s_t)\left(r(s_t,a_t)+\gamma V_\phi^{\pi_\theta}(s_{t+1})-V^{\pi_\theta}_\phi(s_t)\right)\right]
$$

é‚£ä¹ˆï¼Œå¾ˆè‡ªç„¶çš„ä¸€ä¸ªæƒ³æ³•å°±æ˜¯å–

$$
\nabla_\theta J(\theta)=\mathbb{E}_{\tau\sim p_\theta(\tau)}\left[\sum_{t=1}^T \nabla_{\theta}\log \pi_\theta(a_t|s_t)\left[\left(\sum_{t'=t}^{t+n-1} \gamma^{t'-t} r(s_{t'},a_{t'})\right)+\gamma^n V_\phi^{\pi_\theta}(s_{t+n})-V^{\pi_\theta}_\phi(s_t)\right]\right]
$$

è¿™ä¸ªæ–¹æ³•è¢«ç§°ä¸ºN-step returnsã€‚å®ƒä¸ºä»€ä¹ˆåˆç†å‘¢ï¼Ÿæˆ‘ä»¬çœ‹åˆ°ï¼Œå¯¹äºè¾ƒè¿‘çš„å†³ç­–ï¼ˆ $t'\approx t$ ï¼‰ï¼Œå³ä½¿æ˜¯å•æ¬¡é‡‡æ ·æ–¹å·®ä¹Ÿä¸ä¼šå¤ªå¤§ï¼Œå¹¶ä¸”é¿å…äº†biasï¼›è€Œå¯¹äºè¿œå¤„çš„å†³ç­–ï¼Œæˆ‘ä»¬ä½¿ç”¨æ¨¡å‹çš„é¢„æµ‹ç»“æœä»¥é¿å…é‡‡æ ·ã€‚

> Q: ç›´è§‚ä¸Šï¼Œä¸ºä»€ä¹ˆè¾ƒè¿‘çš„å†³ç­–æ–¹å·®å°ï¼Ÿ
>
> A: ä¸å¦¨å¸¦å…¥æˆ‘ä»¬çš„äººç”Ÿã€‚å¦‚æœè€ƒè™‘ä½ æ˜å¤©ä¼šåšä»€ä¹ˆï¼Œå¤šå°‘å¯ä»¥é€šè¿‡ä½ ç°åœ¨çš„è®¡åˆ’æˆ–è€…ä»–äººï¼ˆç¯å¢ƒï¼‰çš„äº‹æƒ…å®‰æ’å†³å®šä¸‹æ¥ï¼›ä½†å€˜è‹¥é—®åˆ°äºŒåå¹´åä½ ä¼šåšä»€ä¹ˆï¼Œé‚£åˆæœ‰è°çœŸæ­£èƒ½ç¡®å®šå‘¢ï¼Ÿ

## Generalized Advantage Estimation

Generalized Advantage Estimationæ–¹æ³•åœ¨å‰é¢çš„N-steps returnçš„åŸºç¡€ä¸Šå†è¿›ä¸€æ­¥ã€‚å¦‚æœä¹‹å‰æˆ‘ä»¬ç»™å‡ºçš„advantage functionæ˜¯

$$
A^{\pi_\theta}_n(s_t,a_t)=\left(\sum_{t'=t}^{t+n-1} \gamma^{t'-t} r(s_{t'},a_{t'})\right)+\gamma^n V_\phi^{\pi_\theta}(s_{t+n})-V^{\pi_\theta}_\phi(s_t)
$$

é‚£ä¹ˆGAEç»™å‡ºçš„advantage functionæ˜¯

$$
A_{\text{GAE}}^{\pi_\theta}(s_t,a_t)=\sum_{n=1}^\infty \lambda^{n-1} A^{\pi_\theta}_n(s_t,a_t)
$$

è¿™æ ·çš„exponential decayçš„æ–¹æ³•å¯ä»¥çœ‹ä½œæ˜¯å¯¹N-step returnçš„ä¸€ä¸ªå¹³æ»‘ã€‚ä¹Ÿå¯ä»¥å±•å¼€ï¼Œä¼šå‘ç°

$$
A_{\text{GAE}}^{\pi_\theta}(s_t,a_t)=\sum_{t'\ge t}(\gamma\lambda)^{t'-t}\left[r(s_{t'},a_{t'})+\gamma V^{\pi_\theta}_\phi(s_{t'+1})-V^{\pi_\theta}_\phi(s_{t'})\right]
$$

å› æ­¤ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥æŠŠ $\gamma\lambda$ æ•´ä½“çœ‹ä½œå‚æ•°ï¼šæ¯ä¸€é¡¹ $r(s_{t'},a_{t'})+\gamma V^{\pi_\theta}_\phi(s_{t'+1})-V^{\pi_\theta}_\phi(s_{t'})$ ä»£è¡¨ç€è¿™ä¸€æ­¥æ˜¯å¦ä¼˜ç§€ï¼›è€Œ $\gamma\lambda$ ä»£è¡¨ç€æˆ‘ä»¬å¯¹æ¯ä¸€æ­¥ä½œå‡ºé‡è¦æ€§å¦‚ä½•éšç€æ—¶é—´è€Œè¡°å‡ã€‚

æœ€åï¼Œå€¼å¾—ä¸€æï¼šGAEæ˜¯ä¸€ä¸ªæ¯”è¾ƒgeneralçš„ç®—æ³•ï¼Œä¹‹å‰çš„ä¸¤ç§æ–¹æ³•éƒ½å¯ä»¥è¢«è§†ä¸ºGAEçš„ç‰¹ä¾‹ã€‚å…·ä½“åœ°ï¼Œå½“ $\lambda=1$ æ—¶ï¼Œå¯ä»¥æ¶ˆå»ä¸­é—´é¡¹å‘ç°GAE advantageæˆä¸ºäº†[state-independent baseline](#state-dependent-baselines)çš„å½¢å¼ï¼›è€Œå½“ $\lambda=0$ æ—¶ï¼ŒGAE advantageæˆä¸ºäº†[vanilla actor-critic](#actor-critic-in-practice)çš„å½¢å¼ã€‚

# Reference Papers

1. [Bias in Natural Actor-Critic Algorithms](https://proceedings.mlr.press/v32/thomas14.html)ï¼ˆè®¨è®ºdiscountçš„é—®é¢˜ï¼‰
2. [Q-Prop: Sample-Efficient Policy Gradient with An Off-Policy Critic](https://arxiv.org/abs/1611.02247)ï¼ˆä¸€ä¸ªcontrol variatesçš„ä¾‹å­ï¼‰
3. [High-Dimensional Continuous Control Using Generalized Advantage Estimation](https://arxiv.org/abs/1506.02438)ï¼ˆGAEç®—æ³•ï¼‰
4. [Asynchronous methods for deep reinforcement learning](https://arxiv.org/abs/1602.01783)ï¼ˆ**A3C**ç®—æ³•ï¼Œä¸€ç§å¹¶è¡Œçš„actor-criticç®—æ³•ï¼‰